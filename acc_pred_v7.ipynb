{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7fb5c1ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fb5c1ed",
        "outputId": "46d5658e-6a6b-4aaa-ebd7-2112096e9d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.3\n",
            "Collecting osmnx\n",
            "  Downloading osmnx-1.9.3-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.2/107.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: geopandas>=0.12 in /usr/local/lib/python3.10/dist-packages (from osmnx) (0.13.2)\n",
            "Requirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from osmnx) (3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from osmnx) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.31.0)\n",
            "Requirement already satisfied: shapely>=2.0 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.0.4)\n",
            "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (1.9.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (24.0)\n",
            "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (8.1.7)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (0.7.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas>=0.12->osmnx) (1.16.0)\n",
            "Installing collected packages: osmnx\n",
            "Successfully installed osmnx-1.9.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install torch-geometric\n",
        "!pip install osmnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b51253c9",
      "metadata": {
        "id": "b51253c9"
      },
      "outputs": [],
      "source": [
        "import osmnx as ox\n",
        "from shapely.geometry import Polygon, Point\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "oXM3e3kL-Cfu",
      "metadata": {
        "id": "oXM3e3kL-Cfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd1ade2-a72a-49b0-fc7d-bf958233dce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the map."
      ],
      "metadata": {
        "id": "oD1fpRj-uUFw"
      },
      "id": "oD1fpRj-uUFw"
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "68ba0344",
      "metadata": {
        "id": "68ba0344"
      },
      "outputs": [],
      "source": [
        "G = ox.load_graphml('/content/drive/MyDrive/RAC Project/map/perth-drive-con-unprojected.graphml')\n",
        "\n",
        "nodes, edges = ox.graph_to_gdfs(G=G,nodes=True,edges=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the historic traffic data and match the road name by ID."
      ],
      "metadata": {
        "id": "oDJqM6L9uV4v"
      },
      "id": "oDJqM6L9uV4v"
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "mhwSK_qKAGIS",
      "metadata": {
        "id": "mhwSK_qKAGIS"
      },
      "outputs": [],
      "source": [
        "traffic_df = pd.read_csv('/content/drive/MyDrive/RAC Project/traffic_data/Total_Traffic_Volume.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b23acf8e",
      "metadata": {
        "id": "b23acf8e"
      },
      "outputs": [],
      "source": [
        "road_df = pd.read_csv('/content/drive/MyDrive/RAC Project/traffic_data/M-Links_Road_Network.csv', usecols=['LINK_DESCR', 'LINK_TO', 'LINK_ID'])\n",
        "road_df['M_Link_ID'] = road_df['LINK_ID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "yon2fIB1AKCB",
      "metadata": {
        "id": "yon2fIB1AKCB"
      },
      "outputs": [],
      "source": [
        "volume_df = traffic_df.merge(road_df[['M_Link_ID', 'LINK_DESCR', 'LINK_TO']], how='left', on='M_Link_ID')\n",
        "volume_df = volume_df[['LINK_DESCR', 'LINK_TO', 'Volumes']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def explode_link_to(df):\n",
        "  df['LINK_TO'] = df['LINK_TO'].apply(lambda x: x.split('&') if isinstance(x, str) else x)\n",
        "  # Explode DataFrame with handling for non-strings\n",
        "  return df.explode('LINK_TO').reset_index(drop=True)\n",
        "\n",
        "volume_df = explode_link_to(volume_df.copy())"
      ],
      "metadata": {
        "id": "7kaxjrXetlse"
      },
      "id": "7kaxjrXetlse",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataframe(df, f1, f2):\n",
        "  df[f1] = df[f1].apply(lambda x: ' '.join(x.split()[:2]) if isinstance(x, str) else x)\n",
        "  df[f2] = df[f2].apply(lambda x: ' '.join(x.split()[:2]) if isinstance(x, str) else x)\n",
        "  return df.copy()\n",
        "\n",
        "volume_df = process_dataframe(volume_df, 'LINK_DESCR', 'LINK_TO')"
      ],
      "metadata": {
        "id": "30OcS2rT2L6E"
      },
      "id": "30OcS2rT2L6E",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the crash information within the study area."
      ],
      "metadata": {
        "id": "qO8stcJyulzr"
      },
      "id": "qO8stcJyulzr"
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "ec606b46",
      "metadata": {
        "id": "ec606b46"
      },
      "outputs": [],
      "source": [
        "crash_df = pd.read_csv('/content/drive/MyDrive/RAC Project/crash_data/Crash_Information.csv',\n",
        "                       usecols=['X', 'Y', 'INTERSECTION_DESC', 'SEVERITY'])\n",
        "crash_df = crash_df.dropna(subset=['INTERSECTION_DESC'])\n",
        "\n",
        "lon_values = [G.nodes[node]['x'] for node in G.nodes()]\n",
        "lat_values = [G.nodes[node]['y'] for node in G.nodes()]\n",
        "\n",
        "min_lon = min(lon_values)\n",
        "max_lon = max(lon_values)\n",
        "min_lat = min(lat_values)\n",
        "max_lat = max(lat_values)\n",
        "\n",
        "filtered_df = crash_df[(crash_df['X'] >= min_lon) &\n",
        "                 (crash_df['X'] <= max_lon) &\n",
        "                 (crash_df['Y'] >= min_lat) &\n",
        "                 (crash_df['Y'] <= max_lat)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the KSI metric."
      ],
      "metadata": {
        "id": "m_pgCnJ1urra"
      },
      "id": "m_pgCnJ1urra"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ksi(group):\n",
        "    ksi_crash = group[(group['SEVERITY'] == 'Fatal') | (group['SEVERITY'] == 'Hospital')].shape[0]\n",
        "    medical_crash = group[group['SEVERITY'] == 'Medical'].shape[0]\n",
        "    casualty_crash = ksi_crash + medical_crash\n",
        "    if casualty_crash == 0:\n",
        "        return 0\n",
        "    ksi_metric = ksi_crash + ksi_crash / casualty_crash * medical_crash\n",
        "    return ksi_metric\n",
        "\n",
        "# Apply the function to each group\n",
        "ksi_metrics = filtered_df.groupby('INTERSECTION_DESC').apply(calculate_ksi).reset_index(name='KSI_metric')\n",
        "\n",
        "# Merge the KSI metric back to the original dataframe\n",
        "filtered_df = filtered_df.merge(ksi_metrics, on='INTERSECTION_DESC', how='left')\n",
        "filtered_df = filtered_df.drop(['SEVERITY'], axis=1)"
      ],
      "metadata": {
        "id": "2xhPsCJo8rJK"
      },
      "id": "2xhPsCJo8rJK",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the traffic volume of each node."
      ],
      "metadata": {
        "id": "1qWDFwsPuu2v"
      },
      "id": "1qWDFwsPuu2v"
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the 'INTERSECTION_DESC' column based on '&' and create new columns\n",
        "filtered_df[['MAJOR_ROAD', 'MINOR_ROAD']] = filtered_df['INTERSECTION_DESC'].str.split('&', n=1, expand=True)\n",
        "\n",
        "# Use explode to expand the DataFrame based on 'INTERSECTION_DESC'\n",
        "expanded_df = filtered_df.assign(MINOR_ROAD=filtered_df['MINOR_ROAD'].str.split('&')).explode('MINOR_ROAD')\n",
        "\n",
        "# Reset index to maintain consecutive row numbers\n",
        "expanded_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "JpjPbqEfAaco"
      },
      "id": "JpjPbqEfAaco",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df = process_dataframe(expanded_df, 'MAJOR_ROAD', 'MINOR_ROAD')"
      ],
      "metadata": {
        "id": "N-5zt-U3BD-q"
      },
      "id": "N-5zt-U3BD-q",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df = expanded_df.drop(['INTERSECTION_DESC'], axis=1)"
      ],
      "metadata": {
        "id": "1YTk1bKgAwQx"
      },
      "id": "1YTk1bKgAwQx",
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = expanded_df.merge(volume_df, left_on=['MAJOR_ROAD', 'MINOR_ROAD'], right_on=['LINK_DESCR', 'LINK_TO'], how='left')\n",
        "merged_df = merged_df.drop(['LINK_DESCR', 'LINK_TO'], axis=1)"
      ],
      "metadata": {
        "id": "cjfeqZr-yTp3"
      },
      "id": "cjfeqZr-yTp3",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Volumes'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "ae6YgiPHpZ8r"
      },
      "id": "ae6YgiPHpZ8r",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = merged_df.groupby(['X', 'Y', 'KSI_metric', 'MAJOR_ROAD', 'MINOR_ROAD']).agg({'Volumes': 'mean'}).reset_index()"
      ],
      "metadata": {
        "id": "CeB34CAIEvJW"
      },
      "id": "CeB34CAIEvJW",
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the crash rate."
      ],
      "metadata": {
        "id": "csKgcw8Bu11K"
      },
      "id": "csKgcw8Bu11K"
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['CRASH_RATE'] = compressed_df['KSI_metric']*10**8/compressed_df['Volumes']/1.7"
      ],
      "metadata": {
        "id": "3z4ak_fX4krX"
      },
      "id": "3z4ak_fX4krX",
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['CRASH_RATE'].fillna(0, inplace=True)\n",
        "compressed_df.loc[compressed_df['Volumes'] == 0, 'CRASH_RATE'] = 0"
      ],
      "metadata": {
        "id": "Tq0R86xH5Dke"
      },
      "id": "Tq0R86xH5Dke",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = compressed_df.drop(['MAJOR_ROAD', 'MINOR_ROAD', 'Volumes'], axis=1)"
      ],
      "metadata": {
        "id": "DhXtz-cu4a7z"
      },
      "id": "DhXtz-cu4a7z",
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['x'] = compressed_df['X']\n",
        "compressed_df['y'] = compressed_df['Y']\n",
        "compressed_df = compressed_df.drop(['X', 'Y'], axis=1)"
      ],
      "metadata": {
        "id": "mSL07sK5HFUw"
      },
      "id": "mSL07sK5HFUw",
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify the risk level (safe, low, medium or high)."
      ],
      "metadata": {
        "id": "gmzrGrAfu6XR"
      },
      "id": "gmzrGrAfu6XR"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the 'CRASH_RATE' column\n",
        "compressed_df[['KSI_metric', 'CRASH_RATE']] = scaler.fit_transform(compressed_df[['KSI_metric', 'CRASH_RATE']])"
      ],
      "metadata": {
        "id": "yXm6H4yjICwc"
      },
      "id": "yXm6H4yjICwc",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = compressed_df[compressed_df['KSI_metric'] != 0]"
      ],
      "metadata": {
        "id": "dUbkHP1G4yjU"
      },
      "id": "dUbkHP1G4yjU",
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ksi_threshold = compressed_df['KSI_metric'].quantile(0.3)\n",
        "crash_rate_threshold = compressed_df['CRASH_RATE'].quantile(0.7)\n",
        "\n",
        "# Classify based on thresholds\n",
        "def classify(row):\n",
        "    if row['KSI_metric'] > ksi_threshold and row['CRASH_RATE'] > crash_rate_threshold:\n",
        "        return 3\n",
        "    elif row['KSI_metric'] > ksi_threshold and row['CRASH_RATE'] <= crash_rate_threshold:\n",
        "        return 2\n",
        "    elif row['KSI_metric'] <= ksi_threshold and row['CRASH_RATE'] > crash_rate_threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "compressed_df['risk_lvl'] = compressed_df.apply(classify, axis=1)\n",
        "\n",
        "label_counts = compressed_df['risk_lvl'].value_counts()\n",
        "label_counts = label_counts.sort_index()\n",
        "label_prob = label_counts / label_counts.sum()"
      ],
      "metadata": {
        "id": "6FGVoQtS7Mba"
      },
      "id": "6FGVoQtS7Mba",
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = compressed_df.drop(['KSI_metric', 'CRASH_RATE'], axis=1)"
      ],
      "metadata": {
        "id": "3NQuymUQIK7F"
      },
      "id": "3NQuymUQIK7F",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the training and testing data."
      ],
      "metadata": {
        "id": "3o_CEua2vDIe"
      },
      "id": "3o_CEua2vDIe"
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "a5b1059d",
      "metadata": {
        "id": "a5b1059d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Convert accident DataFrame to GeoDataFrame\n",
        "accident_geometry = [Point(xy) for xy in zip(compressed_df['x'], compressed_df['y'])]\n",
        "accident_gdf = gpd.GeoDataFrame(compressed_df, geometry=accident_geometry, crs='EPSG:4326')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "f347dd57",
      "metadata": {
        "id": "f347dd57"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "# Convert the x, y coordinates from nodes to a numpy array\n",
        "nodes_array = np.column_stack((nodes['x'], nodes['y']))\n",
        "\n",
        "# Build a BallTree for nearest neighbor search\n",
        "tree = BallTree(nodes_array, leaf_size=15)\n",
        "\n",
        "matched_nodes = accident_gdf.copy()\n",
        "\n",
        "# Define a function to find the nearest point and return its geometry\n",
        "def find_nearest_geometry(row):\n",
        "    point = np.array([[row['x'], row['y']]])\n",
        "    dist, ind = tree.query(point, k=1)\n",
        "    nearest_index = ind[0][0]\n",
        "    return nodes.iloc[nearest_index]['geometry']\n",
        "\n",
        "matched_nodes['geometry'] = matched_nodes.apply(find_nearest_geometry, axis=1)\n",
        "accident_nodes = gpd.GeoDataFrame(matched_nodes, geometry=matched_nodes['geometry'], crs='EPSG:4326')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "fa107ad4",
      "metadata": {
        "id": "fa107ad4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = accident_nodes.drop(columns=['risk_lvl'])\n",
        "y = accident_nodes[['risk_lvl']]\n",
        "train_nodes, test_nodes, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_nodes = gpd.sjoin(nodes, train_nodes, how=\"right\", predicate=\"intersects\")\n",
        "train_nodes = train_nodes[['osmid_original', 'x_right', 'y_right', 'street_count', 'geometry']]\n",
        "train_nodes = train_nodes.rename(columns={'x_right': 'x', 'y_right': 'y'})\n",
        "test_nodes = gpd.sjoin(nodes, test_nodes, how=\"right\", predicate=\"intersects\")\n",
        "test_nodes = test_nodes[['osmid_original', 'x_right', 'y_right', 'street_count', 'geometry']]\n",
        "test_nodes = test_nodes.rename(columns={'x_right': 'x', 'y_right': 'y'})"
      ],
      "metadata": {
        "id": "LC1JsZ7jDc62"
      },
      "id": "LC1JsZ7jDc62",
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "549a3b26",
      "metadata": {
        "id": "549a3b26"
      },
      "outputs": [],
      "source": [
        "train_edges = gpd.sjoin(edges, train_nodes, how=\"inner\", predicate=\"intersects\")\n",
        "train_edges = train_edges[['osmid', 'highway', 'oneway', 'length', 'speed_kph', 'geometry']]\n",
        "test_edges = gpd.sjoin(edges, test_nodes, how=\"inner\", predicate=\"intersects\")\n",
        "test_edges = test_edges[['osmid', 'highway', 'oneway', 'length', 'speed_kph', 'geometry']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "ae61570f",
      "metadata": {
        "id": "ae61570f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "\n",
        "G_train = nx.Graph(crs='EPSG:4326')\n",
        "for idx, row in train_nodes.iterrows():\n",
        "    G_train.add_node(idx, x=row['x'], y=row['y'])\n",
        "for idx, row in train_edges.iterrows():\n",
        "    G_train.add_edge(row.name[0], row.name[1], oneway=row['oneway'])\n",
        "\n",
        "G_train.add_edges_from(nx.selfloop_edges(G_train))\n",
        "\n",
        "x = torch.tensor(train_nodes[['x', 'y', 'street_count']].values, dtype=torch.float)\n",
        "# Ensure that node indices in the edge index are within the range of the number of nodes\n",
        "train_edges['oneway'] = train_edges['oneway'].astype(float)\n",
        "edge_index = torch.tensor(np.array(list(G_train.edges())).T, dtype=torch.long)\n",
        "edge_index = edge_index.remainder(len(train_nodes))  # Ensure node indices are within bounds\n",
        "edge_attr = torch.tensor(train_edges[['oneway', 'length', 'speed_kph']].values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The architecture of the GCN model."
      ],
      "metadata": {
        "id": "Sl6Du3iDvHrA"
      },
      "id": "Sl6Du3iDvHrA"
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "16b7c223",
      "metadata": {
        "id": "16b7c223"
      },
      "outputs": [],
      "source": [
        "# Define GCN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(3, 128)\n",
        "        self.conv2 = GCNConv(128, 64)\n",
        "        self.conv3 = GCNConv(64, 32)\n",
        "        self.conv4 = GCNConv(32, 4)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, 0.1)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, 0.1)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, 0.1)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    if isinstance(m, GCNConv):\n",
        "        torch.nn.init.xavier_uniform_(m.lin.weight.data)\n",
        "        if m.lin.bias is not None:\n",
        "            m.lin.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "W1DFPnhlYrEC"
      },
      "id": "W1DFPnhlYrEC",
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "8bd912ac",
      "metadata": {
        "id": "8bd912ac"
      },
      "outputs": [],
      "source": [
        "y_train_label = torch.tensor(y_train.values, dtype=torch.int64)\n",
        "y_train_label = torch.unsqueeze(y_train_label, dim=1)\n",
        "y_test_label = torch.tensor(y_test.values, dtype=torch.int64)\n",
        "y_test_label = torch.unsqueeze(y_test_label, dim=1)\n",
        "y_train_label = y_train_label.view(-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "loader = DataLoader([data], batch_size=1)"
      ],
      "metadata": {
        "id": "Aw8V99wnS3Qk"
      },
      "id": "Aw8V99wnS3Qk",
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and optimizer\n",
        "gcn_model = GCN()\n",
        "gcn_model.apply(weights_init)\n",
        "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "kXBFfExFOUGG"
      },
      "id": "kXBFfExFOUGG",
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights = 1.0 / torch.tensor(label_prob)\n",
        "class_weights = class_weights / class_weights.sum()  # Normalize\n",
        "\n",
        "class_weights = class_weights.to(torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "3zXPTjkTG4Sj"
      },
      "id": "3zXPTjkTG4Sj",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model = torch.load('/content/drive/MyDrive/RAC Project/model/risk_lvl_5.pth')"
      ],
      "metadata": {
        "id": "PVYsbjaQV6Mi"
      },
      "id": "PVYsbjaQV6Mi",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "7b816b8c",
      "metadata": {
        "id": "7b816b8c"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "gcn_model.train()\n",
        "for epoch in range(100000):\n",
        "    optimizer.zero_grad()\n",
        "    for data in loader:\n",
        "        out = gcn_model(data)\n",
        "\n",
        "        loss = F.cross_entropy(out, y_train_label, weight=class_weights)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1)%1000 == 0:\n",
        "        torch.save(gcn_model, '/content/drive/MyDrive/RAC Project/model/risk_lvl_5.pth')\n",
        "        print(\"Epoch \"+str(epoch+1)+\", loss: \"+str(loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "id": "1ef918d4",
      "metadata": {
        "id": "1ef918d4"
      },
      "outputs": [],
      "source": [
        "# G_test = nx.Graph(crs='EPSG:4326')\n",
        "# # G_test.crs\n",
        "# for idx, row in test_nodes.iterrows():\n",
        "#     G_test.add_node(idx, x=row['x'], y=row['y'])\n",
        "# for idx, row in test_edges.iterrows():\n",
        "#     G_test.add_edge(row.name[0], row.name[1], oneway=row['oneway'])\n",
        "\n",
        "# G_test.add_edges_from(nx.selfloop_edges(G_test))\n",
        "\n",
        "# x_test = torch.tensor(test_nodes[['x', 'y', 'street_count']].values, dtype=torch.float)\n",
        "# test_edges['oneway'] = test_edges['oneway'].astype(float)\n",
        "# edge_index_test = torch.tensor(np.array(list(G_test.edges())).T, dtype=torch.long)\n",
        "# edge_index_test = edge_index_test.remainder(len(test_nodes))  # Ensure node indices are within bounds\n",
        "# edge_attr_test = torch.tensor(test_edges[['oneway', 'length', 'speed_kph']].values, dtype=torch.float)\n",
        "\n",
        "# # Create DataLoader for test data\n",
        "# test_data = Data(x=x_test, edge_index=edge_index_test, edge_attr=edge_attr_test)\n",
        "# test_loader = DataLoader([test_data], batch_size=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict the risk level of all the nodes in our study area."
      ],
      "metadata": {
        "id": "Jo7XxbbSvQjT"
      },
      "id": "Jo7XxbbSvQjT"
    },
    {
      "cell_type": "code",
      "source": [
        "x_perth = torch.tensor(nodes[['x', 'y', 'street_count']].values, dtype=torch.float)\n",
        "perth_edges = edges.copy()\n",
        "perth_edges['oneway'] = perth_edges['oneway'].astype(float)\n",
        "edge_index_perth = torch.tensor(np.array(list(G.edges())).T, dtype=torch.long)\n",
        "edge_index_perth = edge_index_perth.remainder(len(nodes))  # Ensure node indices are within bounds\n",
        "edge_attr_perth = torch.tensor(perth_edges[['oneway', 'length', 'speed_kph']].values, dtype=torch.float)\n",
        "\n",
        "# Create DataLoader for the entire perth\n",
        "perth_data = Data(x=x_perth, edge_index=edge_index_perth, edge_attr=edge_attr_perth)\n",
        "perth_loader = DataLoader([perth_data], batch_size=1)"
      ],
      "metadata": {
        "id": "rAWKtXOtgutd"
      },
      "id": "rAWKtXOtgutd",
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "5e4c7ee6",
      "metadata": {
        "id": "5e4c7ee6"
      },
      "outputs": [],
      "source": [
        "gcn_model.eval()\n",
        "\n",
        "# Perform predictions\n",
        "predictions = []\n",
        "for data in perth_loader:\n",
        "    with torch.no_grad():\n",
        "        out = gcn_model(data)\n",
        "        predicted_labels = torch.argmax(out, dim=1)\n",
        "        # Append the predicted labels for the entire batch to the predictions list\n",
        "        predictions.extend(predicted_labels.cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the risk level into the street network map and save (graph and csv)."
      ],
      "metadata": {
        "id": "ySErHki3vWdW"
      },
      "id": "ySErHki3vWdW"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_series = pd.Series(predictions, index=nodes.index)\n",
        "nodes['risk_lvl'] = predictions_series"
      ],
      "metadata": {
        "id": "s_YgC8Mtglr6"
      },
      "id": "s_YgC8Mtglr6",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to a graphml\n",
        "pred_G = ox.graph_from_gdfs(nodes, edges)\n",
        "ox.save_graphml(pred_G, '/content/drive/MyDrive/RAC Project/map/perth-drive-con-unprojected-risk.graphml')"
      ],
      "metadata": {
        "id": "hbk9ILrHhkUw"
      },
      "id": "hbk9ILrHhkUw",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_columns = nodes[['osmid_original', 'risk_lvl']]\n",
        "selected_columns = selected_columns.rename(columns={'osmid_original': 'osmid'})\n",
        "\n",
        "# Save to a CSV file\n",
        "output_csv_path = '/content/drive/MyDrive/RAC Project/map/risk_lvl.csv'\n",
        "selected_columns.to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "tdviAszfiso6"
      },
      "id": "tdviAszfiso6",
      "execution_count": 95,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}