{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb5c1ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fb5c1ed",
        "outputId": "9f00a534-ee4c-4859-e094-61ee6084e6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Collecting aiohttp (from torch-geometric)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->torch-geometric)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->torch-geometric)\n",
            "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5 (from aiohttp->torch-geometric)\n",
            "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->torch-geometric)\n",
            "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0 (from aiohttp->torch-geometric)\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.5.0)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, torch-geometric\n",
            "Successfully installed aiohttp-3.9.5 aiosignal-1.3.1 async-timeout-4.0.3 frozenlist-1.4.1 multidict-6.0.5 torch-geometric-2.5.3 yarl-1.9.4\n",
            "Collecting osmnx\n",
            "  Downloading osmnx-1.9.3-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.2/107.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting geopandas>=0.12 (from osmnx)\n",
            "  Downloading geopandas-0.14.4-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.5 in /usr/local/lib/python3.10/dist-packages (from osmnx) (3.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from osmnx) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.27 in /usr/local/lib/python3.10/dist-packages (from osmnx) (2.31.0)\n",
            "Collecting shapely>=2.0 (from osmnx)\n",
            "  Downloading shapely-2.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fiona>=1.8.21 (from geopandas>=0.12->osmnx)\n",
            "  Downloading fiona-1.9.6-cp310-cp310-manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas>=0.12->osmnx) (24.0)\n",
            "Collecting pyproj>=3.3.0 (from geopandas>=0.12->osmnx)\n",
            "  Downloading pyproj-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1->osmnx) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.27->osmnx) (2024.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas>=0.12->osmnx) (23.2.0)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas>=0.12->osmnx) (8.1.7)\n",
            "Collecting click-plugins>=1.0 (from fiona>=1.8.21->geopandas>=0.12->osmnx)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting cligj>=0.5 (from fiona>=1.8.21->geopandas>=0.12->osmnx)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.21->geopandas>=0.12->osmnx) (1.16.0)\n",
            "Installing collected packages: shapely, pyproj, cligj, click-plugins, fiona, geopandas, osmnx\n",
            "Successfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.6 geopandas-0.14.4 osmnx-1.9.3 pyproj-3.6.1 shapely-2.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n",
        "!pip install torch-geometric\n",
        "!pip install osmnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51253c9",
      "metadata": {
        "id": "b51253c9"
      },
      "outputs": [],
      "source": [
        "import osmnx as ox\n",
        "from shapely.geometry import Polygon, Point\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oXM3e3kL-Cfu",
      "metadata": {
        "id": "oXM3e3kL-Cfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9b9def-a44b-4441-a01a-f689a7a18d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the map."
      ],
      "metadata": {
        "id": "uzUL-1nZ-DgH"
      },
      "id": "uzUL-1nZ-DgH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68ba0344",
      "metadata": {
        "id": "68ba0344"
      },
      "outputs": [],
      "source": [
        "G = ox.load_graphml('/content/drive/MyDrive/RAC Project/map/perth-drive-con-unprojected.graphml')\n",
        "\n",
        "nodes, edges = ox.graph_to_gdfs(G=G,nodes=True,edges=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data."
      ],
      "metadata": {
        "id": "FHkJtoau-G1d"
      },
      "id": "FHkJtoau-G1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b23acf8e",
      "metadata": {
        "id": "b23acf8e"
      },
      "outputs": [],
      "source": [
        "road_df = pd.read_csv('/content/drive/MyDrive/RAC Project/traffic_data/M-Links_Road_Network.csv', usecols=['LINK_DESCR', 'LINK_TO', 'LINK_ID'])\n",
        "road_df['M_Link_ID'] = road_df['LINK_ID']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mhwSK_qKAGIS",
      "metadata": {
        "id": "mhwSK_qKAGIS"
      },
      "outputs": [],
      "source": [
        "traffic_df = pd.read_csv('/content/drive/MyDrive/RAC Project/traffic_data/Total_Traffic_Volume.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yon2fIB1AKCB",
      "metadata": {
        "id": "yon2fIB1AKCB"
      },
      "outputs": [],
      "source": [
        "volume_df = traffic_df.merge(road_df[['M_Link_ID', 'LINK_DESCR', 'LINK_TO']], how='left', on='M_Link_ID')\n",
        "volume_df = volume_df[['LINK_DESCR', 'LINK_TO', 'Volumes']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def explode_link_to(df):\n",
        "  df['LINK_TO'] = df['LINK_TO'].apply(lambda x: x.split('&') if isinstance(x, str) else x)\n",
        "  # Explode DataFrame with handling for non-strings\n",
        "  return df.explode('LINK_TO').reset_index(drop=True)\n",
        "\n",
        "volume_df = explode_link_to(volume_df.copy())"
      ],
      "metadata": {
        "id": "7kaxjrXetlse"
      },
      "id": "7kaxjrXetlse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataframe(df, f1, f2):\n",
        "  df[f1] = df[f1].apply(lambda x: ' '.join(x.split()[:2]) if isinstance(x, str) else x)\n",
        "  df[f2] = df[f2].apply(lambda x: ' '.join(x.split()[:2]) if isinstance(x, str) else x)\n",
        "  return df.copy()\n",
        "\n",
        "volume_df = process_dataframe(volume_df, 'LINK_DESCR', 'LINK_TO')"
      ],
      "metadata": {
        "id": "30OcS2rT2L6E"
      },
      "id": "30OcS2rT2L6E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec606b46",
      "metadata": {
        "id": "ec606b46"
      },
      "outputs": [],
      "source": [
        "crash_df = pd.read_csv('/content/drive/MyDrive/RAC Project/crash_data/Crash_Information.csv',\n",
        "                       usecols=['X', 'Y', 'INTERSECTION_DESC', 'SEVERITY'])\n",
        "crash_df = crash_df.dropna(subset=['INTERSECTION_DESC'])\n",
        "\n",
        "lon_values = [G.nodes[node]['x'] for node in G.nodes()]\n",
        "lat_values = [G.nodes[node]['y'] for node in G.nodes()]\n",
        "\n",
        "min_lon = min(lon_values)\n",
        "max_lon = max(lon_values)\n",
        "min_lat = min(lat_values)\n",
        "max_lat = max(lat_values)\n",
        "\n",
        "filtered_df = crash_df[(crash_df['X'] >= min_lon) &\n",
        "                 (crash_df['X'] <= max_lon) &\n",
        "                 (crash_df['Y'] >= min_lat) &\n",
        "                 (crash_df['Y'] <= max_lat)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ksi(group):\n",
        "    ksi_crash = group[(group['SEVERITY'] == 'Fatal') | (group['SEVERITY'] == 'Hospital')].shape[0]\n",
        "    medical_crash = group[group['SEVERITY'] == 'Medical'].shape[0]\n",
        "    casualty_crash = ksi_crash + medical_crash\n",
        "    if casualty_crash == 0:\n",
        "        return 0\n",
        "    ksi_metric = ksi_crash + ksi_crash / casualty_crash * medical_crash\n",
        "    return ksi_metric\n",
        "\n",
        "# Apply the function to each group\n",
        "ksi_metrics = filtered_df.groupby('INTERSECTION_DESC').apply(calculate_ksi).reset_index(name='KSI_metric')\n",
        "\n",
        "# Merge the KSI metric back to the original dataframe\n",
        "filtered_df = filtered_df.merge(ksi_metrics, on='INTERSECTION_DESC', how='left')\n",
        "filtered_df = filtered_df.drop(['SEVERITY'], axis=1)"
      ],
      "metadata": {
        "id": "2xhPsCJo8rJK"
      },
      "id": "2xhPsCJo8rJK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the 'INTERSECTION_DESC' column based on '&' and create new columns\n",
        "filtered_df[['MAJOR_ROAD', 'MINOR_ROAD']] = filtered_df['INTERSECTION_DESC'].str.split('&', n=1, expand=True)\n",
        "\n",
        "# Use explode to expand the DataFrame based on 'INTERSECTION_DESC'\n",
        "expanded_df = filtered_df.assign(MINOR_ROAD=filtered_df['MINOR_ROAD'].str.split('&')).explode('MINOR_ROAD')\n",
        "\n",
        "# Reset index to maintain consecutive row numbers\n",
        "expanded_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "JpjPbqEfAaco"
      },
      "id": "JpjPbqEfAaco",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df = process_dataframe(expanded_df, 'MAJOR_ROAD', 'MINOR_ROAD')"
      ],
      "metadata": {
        "id": "N-5zt-U3BD-q"
      },
      "id": "N-5zt-U3BD-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_df = expanded_df.drop(['CRASH_DATE', 'INTERSECTION_DESC'], axis=1)"
      ],
      "metadata": {
        "id": "1YTk1bKgAwQx"
      },
      "id": "1YTk1bKgAwQx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = expanded_df.merge(volume_df, left_on=['MAJOR_ROAD', 'MINOR_ROAD'], right_on=['LINK_DESCR', 'LINK_TO'], how='left')\n",
        "merged_df = merged_df.drop(['LINK_DESCR', 'LINK_TO'], axis=1)"
      ],
      "metadata": {
        "id": "cjfeqZr-yTp3"
      },
      "id": "cjfeqZr-yTp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df['Volumes'].fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "ae6YgiPHpZ8r"
      },
      "id": "ae6YgiPHpZ8r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = merged_df.groupby(['X', 'Y', 'KSI_metric', 'MAJOR_ROAD', 'MINOR_ROAD', 'CRASH_TIME']).agg({'Volumes': 'mean'}).reset_index()"
      ],
      "metadata": {
        "id": "CeB34CAIEvJW"
      },
      "id": "CeB34CAIEvJW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['CRASH_RATE'] = compressed_df['KSI_metric']*10**8/compressed_df['Volumes']/1.7"
      ],
      "metadata": {
        "id": "3z4ak_fX4krX"
      },
      "id": "3z4ak_fX4krX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['CRASH_RATE'].fillna(0, inplace=True)\n",
        "compressed_df.loc[compressed_df['Volumes'] == 0, 'CRASH_RATE'] = 0"
      ],
      "metadata": {
        "id": "Tq0R86xH5Dke"
      },
      "id": "Tq0R86xH5Dke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = compressed_df.drop(['MAJOR_ROAD', 'MINOR_ROAD', 'Volumes'], axis=1)"
      ],
      "metadata": {
        "id": "DhXtz-cu4a7z"
      },
      "id": "DhXtz-cu4a7z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['x'] = compressed_df['X']\n",
        "compressed_df['y'] = compressed_df['Y']\n",
        "compressed_df = compressed_df.drop(['X', 'Y'], axis=1)"
      ],
      "metadata": {
        "id": "mSL07sK5HFUw"
      },
      "id": "mSL07sK5HFUw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "# Normalize 'KSI_METRIC' and 'CRASH_RATE'\n",
        "compressed_df[['KSI_metric', 'CRASH_RATE']] = scaler.fit_transform(compressed_df[['KSI_metric', 'CRASH_RATE']])"
      ],
      "metadata": {
        "id": "yXm6H4yjICwc"
      },
      "id": "yXm6H4yjICwc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df['acc_prob'] = (compressed_df['KSI_metric'] + compressed_df['CRASH_RATE']) / \\\n",
        "                        (compressed_df['KSI_metric'] + compressed_df['CRASH_RATE']).clip(lower=1)"
      ],
      "metadata": {
        "id": "-YZ_sveHIISE"
      },
      "id": "-YZ_sveHIISE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = compressed_df.drop(['KSI_metric', 'CRASH_RATE'], axis=1)"
      ],
      "metadata": {
        "id": "3NQuymUQIK7F"
      },
      "id": "3NQuymUQIK7F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_df = compressed_df.drop(['CRASH_TIME'], axis=1)\n",
        "compressed_df = compressed_df.drop_duplicates(subset=['x', 'y'])"
      ],
      "metadata": {
        "id": "_ewfe-OmWn5N"
      },
      "id": "_ewfe-OmWn5N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b1059d",
      "metadata": {
        "id": "a5b1059d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "# Convert accident DataFrame to GeoDataFrame\n",
        "accident_geometry = [Point(xy) for xy in zip(compressed_df['x'], compressed_df['y'])]\n",
        "accident_gdf = gpd.GeoDataFrame(compressed_df, geometry=accident_geometry, crs='EPSG:4326')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f347dd57",
      "metadata": {
        "id": "f347dd57"
      },
      "outputs": [],
      "source": [
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "# Convert the x, y coordinates from nodes to a numpy array\n",
        "nodes_array = np.column_stack((nodes['x'], nodes['y']))\n",
        "\n",
        "# Build a BallTree for nearest neighbor search\n",
        "tree = BallTree(nodes_array, leaf_size=15)\n",
        "\n",
        "matched_nodes = accident_gdf.copy()\n",
        "\n",
        "# Define a function to find the nearest point and return its geometry\n",
        "def find_nearest_geometry(row):\n",
        "    point = np.array([[row['x'], row['y']]])\n",
        "    dist, ind = tree.query(point, k=1)\n",
        "    nearest_index = ind[0][0]\n",
        "    return nodes.iloc[nearest_index]['geometry']\n",
        "\n",
        "matched_nodes['geometry'] = matched_nodes.apply(find_nearest_geometry, axis=1)\n",
        "accident_nodes = gpd.GeoDataFrame(matched_nodes, geometry=matched_nodes['geometry'], crs='EPSG:4326')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa107ad4",
      "metadata": {
        "id": "fa107ad4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = accident_nodes.drop(columns=['acc_prob'])\n",
        "y = accident_nodes[['acc_prob']]\n",
        "train_nodes, test_nodes, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=33)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549a3b26",
      "metadata": {
        "id": "549a3b26"
      },
      "outputs": [],
      "source": [
        "train_edges = gpd.sjoin(edges, train_nodes, how=\"inner\", predicate=\"intersects\")\n",
        "train_edges = train_edges[['oneway', 'geometry']]\n",
        "test_edges = gpd.sjoin(edges, test_nodes, how=\"inner\", predicate=\"intersects\")\n",
        "test_edges = test_edges[['oneway', 'geometry']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae61570f",
      "metadata": {
        "id": "ae61570f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "import networkx as nx\n",
        "import geopandas as gpd\n",
        "\n",
        "G_train = nx.Graph()\n",
        "for idx, row in train_nodes.iterrows():\n",
        "    G_train.add_node(idx, x=row['x'], y=row['y'])\n",
        "for idx, row in train_edges.iterrows():\n",
        "    G_train.add_edge(row.name[0], row.name[1], oneway=row['oneway'])\n",
        "\n",
        "G_train.add_edges_from(nx.selfloop_edges(G_train))\n",
        "\n",
        "x = torch.tensor(train_nodes[['x', 'y']].values, dtype=torch.float)\n",
        "# Ensure that node indices in the edge index are within the range of the number of nodes\n",
        "edge_index = torch.tensor(np.array(list(G_train.edges())).T, dtype=torch.long)\n",
        "edge_index = edge_index.remainder(len(train_nodes))  # Ensure node indices are within bounds\n",
        "edge_attr = torch.tensor(train_edges['oneway'].values, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16b7c223",
      "metadata": {
        "id": "16b7c223"
      },
      "outputs": [],
      "source": [
        "# Define GNN model\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(2, 128)\n",
        "        self.conv2 = GCNConv(128, 64)\n",
        "        self.conv3 = GCNConv(64, 32)\n",
        "        self.conv4 = GCNConv(32, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, 0.1)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, 0.1)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, 0.1)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bd912ac",
      "metadata": {
        "id": "8bd912ac"
      },
      "outputs": [],
      "source": [
        "y_train_label = torch.tensor(y_train.values, dtype=torch.int64)\n",
        "y_train_label = torch.unsqueeze(y_train_label, dim=1)\n",
        "y_test_label = torch.tensor(y_test.values, dtype=torch.int64)\n",
        "y_test_label = torch.unsqueeze(y_test_label, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "loader = DataLoader([data], batch_size=1)"
      ],
      "metadata": {
        "id": "Aw8V99wnS3Qk"
      },
      "id": "Aw8V99wnS3Qk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model and optimizer\n",
        "gcn_model = GCN()\n",
        "optimizer = torch.optim.Adam(gcn_model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "kXBFfExFOUGG"
      },
      "id": "kXBFfExFOUGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b816b8c",
      "metadata": {
        "id": "7b816b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babfb238-b7e3-46ab-8b2a-f74f231c89d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-256-9688786440df>:9: UserWarning: Using a target size (torch.Size([8044, 1, 1])) that is different to the input size (torch.Size([8044, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  loss = F.l1_loss(out, y_train_label)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100, loss: tensor(0.2597, grad_fn=<MeanBackward0>)\n",
            "Epoch 200, loss: tensor(0.2324, grad_fn=<MeanBackward0>)\n",
            "Epoch 300, loss: tensor(0.2067, grad_fn=<MeanBackward0>)\n",
            "Epoch 400, loss: tensor(0.1832, grad_fn=<MeanBackward0>)\n",
            "Epoch 500, loss: tensor(0.1616, grad_fn=<MeanBackward0>)\n",
            "Epoch 600, loss: tensor(0.1420, grad_fn=<MeanBackward0>)\n",
            "Epoch 700, loss: tensor(0.1242, grad_fn=<MeanBackward0>)\n",
            "Epoch 800, loss: tensor(0.1080, grad_fn=<MeanBackward0>)\n",
            "Epoch 900, loss: tensor(0.0933, grad_fn=<MeanBackward0>)\n",
            "Epoch 1000, loss: tensor(0.0802, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "gcn_model.train()\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    for data in loader:\n",
        "        out = gcn_model(data)\n",
        "\n",
        "        # Define MAE loss\n",
        "        loss = F.l1_loss(out, y_train_label)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1)%100 == 0:\n",
        "        torch.save(gcn_model, '/content/drive/MyDrive/RAC Project/model/prob.pth')\n",
        "        print(\"Epoch \"+str(epoch+1)+\", loss: \"+str(loss))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(gcn_model, '/content/drive/MyDrive/RAC Project/model/prob.pth')"
      ],
      "metadata": {
        "id": "_HpNOk2UOaC1"
      },
      "id": "_HpNOk2UOaC1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef918d4",
      "metadata": {
        "id": "1ef918d4"
      },
      "outputs": [],
      "source": [
        "G_test = nx.Graph(crs='EPSG:4326')\n",
        "# G_test.crs\n",
        "for idx, row in test_nodes.iterrows():\n",
        "    G_test.add_node(idx, x=row['x'], y=row['y'])\n",
        "for idx, row in test_edges.iterrows():\n",
        "    G_test.add_edge(row.name[0], row.name[1], oneway=row['oneway'])\n",
        "\n",
        "G_test.add_edges_from(nx.selfloop_edges(G_test))\n",
        "\n",
        "x_test = torch.tensor(test_nodes[['x', 'y']].values, dtype=torch.float)\n",
        "edge_index_test = torch.tensor(np.array(list(G_test.edges())).T, dtype=torch.long)\n",
        "edge_index_test = edge_index_test.remainder(len(test_nodes))  # Ensure node indices are within bounds\n",
        "edge_attr_test = torch.tensor(test_edges['oneway'].values, dtype=torch.float)\n",
        "\n",
        "# Create DataLoader for test data\n",
        "test_data = Data(x=x_test, edge_index=edge_index_test, edge_attr=edge_attr_test)\n",
        "test_loader = DataLoader([test_data], batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4c7ee6",
      "metadata": {
        "id": "5e4c7ee6"
      },
      "outputs": [],
      "source": [
        "gcn_model.eval()\n",
        "\n",
        "# Perform predictions\n",
        "predictions = []\n",
        "for data in test_loader:\n",
        "    with torch.no_grad():\n",
        "        out = gcn_model(data)\n",
        "        predictions.append(out)\n",
        "\n",
        "# Convert predictions to numpy array\n",
        "predictions = torch.cat(predictions).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = np.sum(predictions == y_test)\n",
        "\n",
        "# Calculate the total number of samples\n",
        "total_samples = len(predictions)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = num_correct / total_samples\n",
        "\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "VB3Wodq_Nb23"
      },
      "id": "VB3Wodq_Nb23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "f1 = f1_score(y_test_values, predictions, average='weighted')  # You can also use 'micro' or 'weighted'\n",
        "\n",
        "print(f'F1-score: {f1}')"
      ],
      "metadata": {
        "id": "umWy2pdDpDWC"
      },
      "id": "umWy2pdDpDWC",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}